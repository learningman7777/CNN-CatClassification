{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatClassification\n",
    "회사에서 열렸던 고양이 분류 강좌 실습 내용기반의 내용으로 작성되었습니다.\n",
    "<br>설명은 제가 복습을 위해 임의로 단 것입니다.\n",
    "<br>고양이 사진마다 \"품종\"에 대한 label을 달아 학습 시킨 후,\n",
    "<br>어떤 고양이 이미지를 넣었을 때, 그 이미지 속 고양이의 품종을 맞추는 것을 목표로 합니다.\n",
    "<br>학습 방식은 supervised learning입니다.\n",
    "<br>먼저 필요한 라이브러리를 설치하고, import 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install --no-cache-dir -I pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparation training and validation data\n",
    "\n",
    "http://www.robots.ox.ac.uk/~vgg/data/pets/   \n",
    "이미지는 위 링크에서 다운로드 가능하고, 압축을 풀어 images/ 폴더에 넣어줍니다.  \n",
    "기본적으로 \"품종_번호.jpg\"와 형식의 파일명을 갖고 있습니다.  \n",
    "아래 코드 실행 시 고양이 이미지가 잘 나오면 됩니다.  \n",
    "숫자를 변경해보면서 다른 이미지도 잘 불러와 지는지 확인하면 좋습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image.open(\"images/Abyssinian_5.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본격적인 학습을 위해, 먼저 이미지 파일명에 적혀있는 \"품종\"이름을 기준으로 데이터를 구분해놓습니다.  \n",
    "\n",
    "<p>\n",
    "아래 코드가 실행 되면,  \n",
    "<br>data  \n",
    "<br>ㄴTRAIN\n",
    "<br>&nbsp;&nbsp;&nbsp;&nbsp;ㄴAbyssinian  \n",
    "<br>&nbsp;&nbsp;&nbsp;&nbsp;ㄴamerican_bulldog  \n",
    "<br>&nbsp;&nbsp;&nbsp;&nbsp;....  \n",
    "<br>ㄴVAL  \n",
    "<br>&nbsp;&nbsp;&nbsp;&nbsp;ㄴAbyssinian  \n",
    "<br>&nbsp;&nbsp;&nbsp;&nbsp;ㄴamerican_bulldog  \n",
    "<br>&nbsp;&nbsp;&nbsp;&nbsp;....  \n",
    "<br>이런 형태의 품종별 폴더 구조로 이미지가 분류됩니다.  \n",
    "\n",
    "이중 sklearn.model_selection.train_test_split함수는 데이터를 training용, validation용으로 분류해주는 함수입니다.  \n",
    "여러 개의 배열을 매개변수로 넣고, stratify를 정해주면, \n",
    "stratify에 지정된 배열의 인덱스를 기준으로 training/validation을 분류해주고, \n",
    "나눠진 결과물을 t/v/t/v 순으로 리턴해줍니다.  \n",
    "<br>\n",
    "아래 코드에서는 이미지 경로와 레이블을 넣었으므로,  \n",
    "train_x에는 training용으로 설정된 이미지의 경로  \n",
    "val_x에는 validation용으로 설정된 이미지의 경로  \n",
    "train_y에는 training용으로 설정된 이미지의 라벨  \n",
    "val_y에는 validation용으로 설정된 이미지의 라벨  \n",
    "이 각각 채워지게 됩니다.\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# helper function for dataset preparation\n",
    "def extract_label(img_path):\n",
    "  \"\"\"\n",
    "  Extract label from \"image path\"\n",
    "  \n",
    "  arguments:\n",
    "  img_path (string): ex)\"images/Egyptian_Mau_63.jpg\"\n",
    "  \n",
    "  return:\n",
    "  label (string): ex)Egyption_Mau\n",
    "  \"\"\"\n",
    "  label = re.search(r'images\\\\(.*)(_(.+).jpg)', img_path).group(1)\n",
    "  return label\n",
    "\n",
    "\n",
    "# helper function for dataset preparation\n",
    "def copy_files(x, y, dataset_dir, category):\n",
    "  print(\">> copying files to {}\".format(category))\n",
    "  for image_path, label in zip(x, y):\n",
    "    \n",
    "    # if label directory not exist then make it.\n",
    "    new_img_dir = os.path.join(dataset_dir, category, label)\n",
    "    if not os.path.exists(new_img_dir):\n",
    "      os.makedirs(new_img_dir)\n",
    "    \n",
    "    # copy image\n",
    "    new_image_path = os.path.join(new_img_dir, os.path.basename(image_path))\n",
    "    shutil.copy(image_path, new_image_path)\n",
    "  \n",
    "  print(\">> {} dataset created.\".format(category))\n",
    "\n",
    "  \n",
    "RAW_DATA_DIR = 'images'\n",
    "def prepare_images(raw_data_dir):\n",
    "  print(\">> preparing image data..\")\n",
    "  \n",
    "  # 이미지 파일을 찾아서 레이블 값을 추출합니다.\n",
    "  image_paths = glob.glob(\"{}/*.jpg\".format(raw_data_dir))\n",
    "  image_df = pd.DataFrame(image_paths)\n",
    "  image_df.columns = ['img_path']\n",
    "  image_df['label'] = image_df.img_path.map(extract_label)\n",
    "\n",
    "  # 데이터셋을 트레이닝 데이터와 밸리데이션 데이터로 나눕니다. (이미지 경로, 레이블)\n",
    "  print(\">> spliting training / validation dataset\")\n",
    "  train_x, val_x, train_y, val_y = train_test_split(\n",
    "      image_df.img_path, image_df.label, # 분리할 이미지 경로와 레이블\n",
    "      stratify=image_df.label, # 레이블을 기준으로 분할\n",
    "      random_state=42, # 랜덤 시드\n",
    "      test_size=0.1 # 밸리데이션 데이터 비율\n",
    "  )\n",
    "\"\"\"\n",
    "train_x에는 training용으로 설정된 이미지의 경로  \n",
    "val_x에는 validation용으로 설정된 이미지의 경로  \n",
    "train_y에는 training용으로 설정된 이미지의 라벨  \n",
    "val_y에는 validation용으로 설정된 이미지의 라벨\n",
    "\"\"\"   \n",
    "  \n",
    "  # 데이터셋 폴더를 만듭니다.\n",
    "  if not os.path.exists(DATASET_DIR):\n",
    "    os.makedirs(DATASET_DIR)\n",
    "    \n",
    "  # 트레이닝 데이터셋을 만듭니다.\n",
    "  copy_files(train_x, train_y, DATASET_DIR, 'TRAIN')\n",
    "  \n",
    "  # 벨리데이션 데이터셋을 만듭니다.\n",
    "  copy_files(val_x, val_y, DATASET_DIR, 'VAL')\n",
    "  \n",
    "  print(\">> job done.\")\n",
    "  \n",
    "\n",
    "DATASET_DIR = \"data\"\n",
    "if not os.path.exists(DATASET_DIR):\n",
    "  print(\"preparing image data\")\n",
    "  prepare_images(RAW_DATA_DIR)\n",
    "else:\n",
    "  print(\"image data are already prepared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 만들기\n",
    "아래 코드는 이미지 파일을 불러오고, 이미지를 pytorch에서 사용할 데이터 형태로 만드는 과정입니다.\n",
    "<br>여기서 [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]는 Imagenet에서 수많은 이미지들의 평균 값과 표준편차 값을 미리 구해놓을 수치라고 합니다.\n",
    "\n",
    "ImageFolder클래스에 폴더를 집어 넣으면, raw이미지를 읽어서 데이터셋을 만들어 주는데,\n",
    "<br>이 때, 폴더명이 classname(Supervised Learning에서 Label)이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as vdatasets\n",
    "\n",
    "\n",
    "def create_dataset(dataset_dir):\n",
    "  \n",
    "  # raw image를 가공하여 모델에 넣을 수 있는 인풋으로 변환합니다.\n",
    "  data_transforms = {\n",
    "      'TRAIN': transforms.Compose([\n",
    "          transforms.Resize((224, 224)), # 1. 사이즈를 224, 224로 통일.\n",
    "          transforms.RandomHorizontalFlip(), # 좌우반전으로 데이터셋 2배 뻥튀기\n",
    "          transforms.ToTensor(), # 2. PIL이미지를 숫자 텐서로 변환.\n",
    "          transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) #3. 노멀라이즈\n",
    "      ]),\n",
    "      'VAL': transforms.Compose([\n",
    "          transforms.Resize((224, 224)),\n",
    "          transforms.RandomHorizontalFlip(),\n",
    "          transforms.ToTensor(),\n",
    "          transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "      ])\n",
    "  }\n",
    "  \n",
    "  # 이미지 데이터셋의 형태로 트레이닝과 밸리데이션 데이터셋을 준비합니다.\n",
    "  # 이 ImageFolder클래스에 폴더를 집어 넣으면, raw이미지를 읽어서 데이터셋을 만들어 주는데, \n",
    "  # 이 때, 폴더명이 classname(Supervised Learning에서 Label)이 됩니다.\n",
    "  image_datasets = {x: vdatasets.ImageFolder(os.path.join(dataset_dir, x), data_transforms[x])\n",
    "                      for x in ['TRAIN', 'VAL']}\n",
    "    \n",
    "  nb_classes = len(image_datasets['TRAIN'].classes)\n",
    "  \n",
    "  return image_datasets, nb_classes\n",
    "\n",
    "image_datasets, nb_classes = create_dataset(DATASET_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래 코드를 실행해보면, 이미지 경로와 label이 하나의 데이터 튜플로 되어있음을 알 수 있습니다.\n",
    "# image_datasets['TRAIN'].imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(image_datasets, training_batch_size, validation_batch_size, isShuffle):\n",
    "  dataloaders = {'TRAIN': torch.utils.data.DataLoader(image_datasets['TRAIN'], batch_size=training_batch_size, shuffle=isShuffle),\n",
    "                 'VAL': torch.utils.data.DataLoader(image_datasets['VAL'], batch_size=validation_batch_size, shuffle=isShuffle)\n",
    "                }\n",
    "  return dataloaders\n",
    "\n",
    "dataloaders = create_dataloaders(image_datasets, 32, 32, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets['TRAIN'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터로더가 한번에 밀어내는 이미지와 레이블 배치를 살펴봅시다.\n",
    "inputs, targets = next(iter(dataloaders['TRAIN']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 224, 224])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 32, 3, 224, 224 크기의 텐서가 출력됩니다.\n",
    "# 이 숫자들은 왼쪽부터 미니배치 크기, 이미지 채널 수 (RGB), 이미지 높이, 너비를 의미합니다.\n",
    "inputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1, 16,  1, 20,  0, 11, 16, 28,  0, 22, 11, 17, 29, 24,  0, 18, 20, 16,\n",
       "        29,  2, 16, 15,  1, 14, 16, 33, 25, 36,  8,  3, 16, 19])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# targets에는 32개 이미지들의 라벨이 들어있습니다. supervised learning에서 \"정답\"에 해당합니다.\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 데이터셋에는 `class_to_idx`를 통해 실제 레이블-숫자 맵핑 정보를 얻을 수 있죠.\n",
    "image_datasets['TRAIN'].class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 숫자를 넣으면 그에 대응되는 레이블을 출력하는 딕셔너리를 만들어봅시다.\n",
    "int2label = {v: k for k, v in image_datasets['TRAIN'].class_to_idx.items()}\n",
    "\n",
    "int2label[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss function 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def prepare_loss_function_and_optimizer(lr, model):\n",
    "  loss_function = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "  \n",
    "  return loss_function, optimizer\n",
    "\n",
    "# loss_function, optimizer = prepare_loss_function_and_optimizer(0.001, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train / evaluate 함수 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def train(model, optimizer, loss_function, data_iterator, epoch):\n",
    "  \n",
    "  print(\"training epoch {}\".format(epoch))\n",
    "  # model을 학습 모드로 바꿔줍니다.\n",
    "  model.train()\n",
    "  \n",
    "  nb_corrects = 0\n",
    "  nb_data = 0\n",
    "  loss_list = []\n",
    "  \n",
    "  for ix, (inputs, targets) in enumerate(data_iterator):\n",
    "    inputs = inputs.to(DEVICE)\n",
    "    targets = targets.to(DEVICE)\n",
    "    \n",
    "    # 모델에 inputs를 넣어 출력값 outputs를 얻습니다.\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # 출력값과 실제값의 오차를 계산합니다.\n",
    "    loss = loss_function(outputs, targets)\n",
    "    loss_list.append(loss.item())\n",
    "    \n",
    "    # 실제 맞춘 갯수와 전체 갯수를 업데이트합니다.\n",
    "    nb_corrects += (outputs.argmax(1) == targets).sum().item()\n",
    "    nb_data += len(targets)\n",
    "    \n",
    "    # optimizer를 먼저 깔끔하게 초기화합니다.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # loss를 역전파합니다.\n",
    "    loss.backward()\n",
    "    \n",
    "    # optimizer를 사용해 모델의 파라미터를 업데이트합니다.\n",
    "    optimizer.step()\n",
    "    \n",
    "    if ix % 100 == 0:\n",
    "      print(\">> [{}] | loss: {:.4f}\".format(ix, loss))\n",
    "    \n",
    "  epoch_accuracy = nb_corrects / nb_data\n",
    "  epoch_avg_loss = torch.tensor(loss_list).mean().item()\n",
    "  \n",
    "  print(\"[training {:03d}] avg_loss: {:.4f} | accuracy: {:.4f}\".format(epoch, epoch_avg_loss, epoch_accuracy))\n",
    "    \n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = CNNModel(nb_classes).to(DEVICE)    \n",
    "\n",
    "# train(model, optimizer, loss_function, dataloaders['TRAIN'], 0)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loss_function, data_iterator, epoch):\n",
    "  \n",
    "  print(\"evaluation epoch {}\".format(epoch))\n",
    "  model.eval()\n",
    "  \n",
    "  nb_corrects = 0\n",
    "  nb_data = 0\n",
    "  loss_list = []\n",
    "  \n",
    "  for inputs, targets in data_iterator:\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      inputs = inputs.to(DEVICE)\n",
    "      targets = targets.to(DEVICE)\n",
    "\n",
    "      # 모델에 inputs를 넣어 출력값 outputs를 얻습니다.\n",
    "      outputs = model(inputs)\n",
    "\n",
    "      # 출력값과 실제값의 오차를 계산합니다.\n",
    "      loss = loss_function(outputs, targets)\n",
    "      loss_list.append(loss.item())\n",
    "\n",
    "      # 실제 맞춘 갯수와 전체 갯수를 업데이트합니다.\n",
    "      nb_corrects += (outputs.argmax(1) == targets).sum().item()\n",
    "      nb_data += len(targets)\n",
    "    \n",
    "  epoch_accuracy = nb_corrects / nb_data\n",
    "  epoch_avg_loss = torch.tensor(loss_list).mean().item()\n",
    "  \n",
    "  print(\"[validation {:03d}] avg_loss: {:.4f} | accuracy: {:.4f}\".format(epoch, epoch_avg_loss, epoch_accuracy))\n",
    "    \n",
    "# evaluate(model, loss_function, dataloaders['VAL'], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(image_datasets, model, lr, num_epochs):\n",
    "  dataloaders = create_dataloaders(image_datasets, 32, 32, True)\n",
    "  \n",
    "  # loss_function과 optimizer를 준비합니다.\n",
    "  loss_function, optimizer = prepare_loss_function_and_optimizer(lr, model)\n",
    "  \n",
    "  for epoch in range(num_epochs):\n",
    "    # 트레이닝\n",
    "    train(model, optimizer, loss_function, dataloaders['TRAIN'], epoch)\n",
    "    # 밸리데이션\n",
    "    evaluate(model, loss_function, dataloaders['VAL'], epoch)\n",
    "    \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이런식으로 모델만 생성한 후 함수를 실행하면 됩니다.\n",
    "#DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#model = CNNModel(nb_classes).to(DEVICE)\n",
    "#model = run_all(image_datasets, model, 0.0001, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN 모델 구성해보기\n",
    "\n",
    "- Object Classification을 수행하는 CNN 모델은 Feature Extraction과 Classification 파트로 구성됩니다.\n",
    "- Feature Extraction은 간단히 CONV-RELU-POOL을 여러번 반복해서 구현할 수 있습니다.\n",
    "- Classification은 Feature Extraction의 결과를 37개의 레이블에 연결하는 과정을 거치며 다음과 같은 두가지 방법이 주로 쓰입니다..\n",
    "  - 1) flatten - fully connected layer - softmax\n",
    "  - 2) global average pooling - fully connected layer - softmax\n",
    "  \n",
    "  아래에서는 1번 옵션을 사용해서 CNN 모델을 만들어보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# PyTorch에서 쓰는 여러 방법 중 하나로,\n",
    "# 클래스로 모델을 만들어봅시다.\n",
    "class CNNModel(nn.Module):\n",
    "  \n",
    "  # 여기서 레이어를 정의합니다.\n",
    "  # 보통 학습가능한 파라미터가 들어가는 레이어를 여기서 먼저 정의해주고,\n",
    "  # 아래 `forward`에서 가져다 씁니다.\n",
    "  def __init__(self, nb_classes):\n",
    "    super(CNNModel, self).__init__()\n",
    "    \n",
    "    # conv 레이어를 하나 둡니다. 7x7 필터를 2픽셀씩 건너뛰면서 필터링을 합니다.\n",
    "    self.conv1 = nn.Conv2d(in_channels = 3, \n",
    "                           out_channels = 16, \n",
    "                           kernel_size = 7, \n",
    "                           stride = 2)\n",
    "    \n",
    "    # pool 레이어를 하나 둡니다. conv를 거친 결과를 2x2커널로 2픽셀씩 건너뛰면서 최댓값만 취합니다.\n",
    "    # 이로서 이미지의 가로세로가 절반으로 줄어듭니다.\n",
    "    self.pool1 = nn.MaxPool2d(kernel_size=2,\n",
    "                              stride=2)\n",
    "    \n",
    "    # pool 레이어의 출력물에 ReLU 액티베이션을 넣어줍니다. 음수는 모두 0이 됩니다.\n",
    "    self.relu1 = nn.ReLU()\n",
    "    \n",
    "    # conv1->pool1->relu1을 한번 더 반복합니다.\n",
    "    self.conv2 = nn.Conv2d(in_channels = 16, \n",
    "                           out_channels = 32, \n",
    "                           kernel_size = 3, \n",
    "                           stride = 1)\n",
    "    self.pool2 = nn.MaxPool2d(kernel_size=2,\n",
    "                              stride=2)\n",
    "    self.relu2 = nn.ReLU()\n",
    "    \n",
    "    # 같은 층을 한번 더 반복합니다.\n",
    "    self.conv3 = nn.Conv2d(in_channels = 32, \n",
    "                           out_channels = 64, \n",
    "                           kernel_size = 3, \n",
    "                           stride = 1)\n",
    "    self.pool3 = nn.MaxPool2d(kernel_size=2,\n",
    "                              stride=2)\n",
    "    self.relu3 = nn.ReLU()\n",
    "    \n",
    "    # 같은 층을 한번 더 반복합니다.\n",
    "    self.conv4 = nn.Conv2d(in_channels = 64, \n",
    "                           out_channels = 128, \n",
    "                           kernel_size = 3, \n",
    "                           stride = 1)\n",
    "    self.pool4 = nn.MaxPool2d(kernel_size=2,\n",
    "                              stride=2)\n",
    "    self.relu4 = nn.ReLU()\n",
    "    \n",
    "    \n",
    "    # conv 레이어의 출력물을 fully connected 레이어에 넣어 분류하기 위해서는\n",
    "    # flatten 과정을 거쳐야 합니다.\n",
    "    # conv 레이어의 출력물은 [32, 128, 5, 5]로,\n",
    "    # 이를 [32, 128*5*5], 즉 [32, 3200]로 변환합니다.\n",
    "    # 즉, 하나의 이미지는 3200개의 숫자로 변환되며, 이를\n",
    "    # 아래 2개의 fully connected layer에 통과시킵니다.\n",
    "    \n",
    "    self.fc1 = nn.Linear(in_features = 3200, \n",
    "                         out_features = 512)\n",
    "    self.relu_fc1 = nn.ReLU()\n",
    "    \n",
    "    # 마지막으로 4096개의 숫자를 우리가 가진 클래스만큼의 숫자로 변환해줍니다.\n",
    "    self.fc2 = nn.Linear(512, nb_classes)\n",
    "    \n",
    "    \n",
    "  # forward propagation  \n",
    "  def forward(self, inputs):\n",
    "    \n",
    "    # 1st conv sequence\n",
    "    x = self.conv1(inputs)\n",
    "    x = self.relu1(x)\n",
    "    x = self.pool1(x)\n",
    "    \n",
    "    # 2nd conv sequence\n",
    "    x = self.conv2(x)\n",
    "    x = self.relu2(x)\n",
    "    x = self.pool2(x)\n",
    "    \n",
    "    # 3rd conv sequence\n",
    "    x = self.conv3(x)\n",
    "    x = self.relu3(x)\n",
    "    x = self.pool3(x)\n",
    "    \n",
    "    # 4th conv sequence\n",
    "    x = self.conv4(x)\n",
    "    x = self.relu4(x)\n",
    "    x = self.pool4(x)\n",
    "    \n",
    "    # flatten\n",
    "    minibatch_size = inputs.size(0)\n",
    "    flattened_x = x.view(minibatch_size, -1)\n",
    "    \n",
    "    # 1st fc sequence\n",
    "    x = self.fc1(flattened_x)\n",
    "    x = self.relu_fc1(x)\n",
    "    \n",
    "    # 2nd fc sequence\n",
    "    x = self.fc2(x)\n",
    "    \n",
    "    return x\n",
    "    \n",
    "model = CNNModel(nb_classes)\n",
    "\n",
    "\n",
    "# sample_run\n",
    "inputs, targets = next(iter(dataloaders['TRAIN']))\n",
    "\n",
    "outputs = model(inputs)\n",
    "\n",
    "print(outputs.size())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNModel(nb_classes).to(DEVICE)\n",
    "model = run_all(image_datasets, model, 0.0001, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning해보기\n",
    "Transfer Learning은 이미 검증된 성능과 학습된 가중치를 가진 분류 모델을 이용해서 학습하면, 통상적으로 다른 분야의 분류에서도 금방 좋은 성능을 찾아간다는 개념의 학습 방법입니다. 여기서는 resnet50을 이용해서 학습을 해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as vmodels\n",
    "import copy\n",
    "\n",
    "base_resnet = vmodels.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class Resnet_fc(nn.Module):\n",
    "    def __init__(self, base_model, nb_classes, toFreeze=False):\n",
    "        super(Resnet_fc, self).__init__()\n",
    "        \n",
    "        base_model_copy = copy.deepcopy(base_model)\n",
    "        self.feature_extractor = nn.Sequential(*list(base_model_copy.children())[:-2])\n",
    "        \n",
    "        if toFreeze:\n",
    "          for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad=False\n",
    "        else:\n",
    "          for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad=True\n",
    "          \n",
    "        \n",
    "        self.gap = nn.AvgPool2d(7, 1)\n",
    "        self.linear = nn.Linear(2048, nb_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.feature_extractor(inputs)\n",
    "        x = self.gap(x).squeeze(-1).squeeze(-1)\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_tl_resnet = Resnet_fc(base_resnet, nb_classes, toFreeze=False).to(DEVICE)\n",
    "frozen_tl_resnet = run_all(image_datasets, frozen_tl_resnet, 0.00001, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
